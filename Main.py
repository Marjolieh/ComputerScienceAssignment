# -*- coding: utf-8 -*-
"""Poging snel.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zjAB3_LeqbTHKl7r8pEx3KjWe8ug-Nuz
"""



"""# Data_Processing"""

# -*- coding: utf-8 -*-
"""
Created on Sun Nov 17 13:58:04 2024

@author: 530356mh
"""

import json
import pandas as pd
from collections import Counter
import numpy as np
import re


def clean_data(data):
    """
    Cleans the product title and featuresMap fields, normalizes units, and combines them into a single string.

    Args:
        data (dict): Dictionary containing original product data with titles and featuresMap.

    Returns:
        dict: Cleaned data with normalized fields and a combined representation.
    """
    # Mapping of units to their standardized forms
    unit_mappings = {
        'inch': [r'"', r'\-inch', r' inch', r'inch', r'inches'],
        'hz': [r'Hertz', r'hertz', r'Hz', r'HZ', r' hz', r'-hz', r'hz']
    }



    def normalize_units(text):
        """
        Replaces different variations of units (e.g., Hz, inch) with their standardized form.

        Args:
            text (str): Input text to normalize.

        Returns:
            str: Text with normalized units.
        """
        for normalized, variations in unit_mappings.items():
          for variation in variations:
            text = re.sub(variation, normalized, text, flags=re.IGNORECASE)
        return text

    # Dictionary to store cleaned data
    cleaned_data = {}

    for key, entry in data.items():
        # Clean and normalize the title
        cleaned_title = normalize_units(entry.get("title", ""))
        cleaned_title = re.sub(r'[^a-zA-Z0-9\s]', '', cleaned_title).lower().strip()  # Remove non-alphanumeric characters

        # Clean and normalize the featuresMap
        cleaned_features_map = {
            key: re.sub(r'[^a-zA-Z0-9\s]', '', normalize_units(value)).lower().strip()
            for key, value in entry.get("featuresMap", {}).items()
        }

        # Combine cleaned title and featuresMap values into a single string
        cleaned_combined = cleaned_title + " " + " ".join(cleaned_features_map.values())

        # Add the cleaned data to the dictionary
        cleaned_data[key] = ({
            "shop": entry.get("shop", ""),  # Store shop name
            "title": cleaned_title,         # Store cleaned title
            "featuresMap": cleaned_features_map,  # Store cleaned featuresMap
            "combined": cleaned_combined    # Store combined representation
        })

    return cleaned_data

"""# Signatures

"""

import numpy as np
import re
import random

random.seed(123)  # Set a seed for reproducibility

def extract_model_words(cleaned_data):
    """
    Extracts model words from product titles and featuresMap using regex patterns.

    Args:
        cleaned_data (dict): Cleaned product data containing title and featuresMap.

    Returns:
        set: A set of unique model words extracted from the data.
    """
    # Regex for capturing model-like patterns in titles
    title_pattern = re.compile(r'([a-zA-Z0-9]*(([0-9]+[^0-9, ]+)|([^0-9, ]+[0-9]+))[a-zA-Z0-9]*)')
    # Regex for capturing numeric values with or without units
    value_pattern = re.compile(r'(^\d+(\.\d+)?[a-zA-Z]+$|^\d+(\.\d+)?$)')

    model_words = set()  # Set to store unique model words

    for key, entry in cleaned_data.items():
        # Search for model words in titles
        title_words = entry["title"].split()
        for word in title_words:
            if title_pattern.match(word):  # Add word if it matches the title pattern
                model_words.add(word)

        # Search for model words in featuresMap values
        for value in entry["featuresMap"].values():
            value_words = value.split()
            for word in value_words:
                if value_pattern.match(word):  # Add word if it matches the value pattern
                    model_words.add(word)

    return model_words


def create_binary_vectors(cleaned_data, model_words):
    """
    Creates binary vectors for each product based on the presence of model words.

    Args:
        cleaned_data (dict): Cleaned product data.
        model_words (set): Set of unique model words.

    Returns:
        dict: Binary vectors grouped by product ID.
    """
    # Ensure consistency by sorting model words into a list
    model_words = sorted(model_words)
    binary_vectors = {}

    for key, entry in cleaned_data.items():
        # Create an empty vector for each model word
        vector = np.zeros(len(model_words), dtype=int)
        # Split the combined field into a set of words
        words = set(entry["combined"].split())
        # Check if each model word is present in the product's text
        for i, model_word in enumerate(model_words):
            if model_word in words:
                vector[i] = 1

        # Store the binary vector along with shop information
        binary_vectors[key] = {
            "shop": entry["shop"],
            "binary_vector": vector.tolist()
        }

    return binary_vectors


def create_signatures(cleaned_data, bands):
    """
    Generates MinHash signatures for binary vectors derived from cleaned data.

    Args:
        cleaned_data (dict): Cleaned product data.
        bands (int): Number of bands (currently unused but can be extended).

    Returns:
        dict: MinHash signatures grouped by product ID.
    """
    # Extract model words and create binary vectors
    model_words = extract_model_words(cleaned_data)
    model_words = sorted(model_words)

    # Define the number of hash functions based on model word length
    nbits = int(len(model_words) * 0.5)
    binary_vectors = create_binary_vectors(cleaned_data, model_words)
    print(f"Length of signatures: {nbits}")

    # Generate hash functions for MinHash
    hashes = build_minhash_func(len(model_words), nbits)

    signatures = {}
    for key, entry in binary_vectors.items():
        # Create a MinHash signature for the binary vector
        new_signature = create_sig(entry["binary_vector"], hashes)

        # Store the signature along with shop information
        signatures[key] = {
            "shop": entry.get("shop", ""),
            "signature": new_signature
        }
    return signatures


def build_minhash_func(size, nbits):
    """
    Constructs multiple MinHash functions as permutation vectors.

    Args:
        size (int): Size of the input vector.
        nbits (int): Number of hash functions to create.

    Returns:
        list: List of hash functions represented as permutation vectors.
    """
    hashes = []
    for _ in range(nbits):
        hashes.append(create_hash_func(size))
    return hashes


def create_sig(vector, minhash_func):
    """
    Creates a MinHash signature for a binary vector using the provided hash functions.

    Args:
        vector (list): Binary vector representing the product.
        minhash_func (list): List of MinHash functions (permutation vectors).

    Returns:
        list: MinHash signature for the input vector.
    """
    signature = []
    for func in minhash_func:
        for idx in func:
            if vector[idx] == 1:  # Find the first 1 in the hash order
                signature.append(idx)
                break
    return signature


def create_hash_func(size):
    """
    Creates a single hash function as a permutation vector.

    Args:
        size (int): Size of the input vector.

    Returns:
        list: Permutation vector representing the hash function.
    """
    hash_vec = list(range(size))
    random.shuffle(hash_vec)  # Shuffle to create a random permutation
    return hash_vec

"""# LSH"""

import numpy as np
from collections import defaultdict
import math

def id_create_signature(signatures):
    """
    Creates a mapping from product identifiers to their respective signatures.

    Args:
        signatures (dict): A dictionary containing product IDs and their signatures.

    Returns:
        dict: A dictionary mapping product identifiers to their MinHash signatures.
    """
    identifier_to_signature = {}
    for key, entry in signatures.items():
        identifier_to_signature[key] = entry["signature"]
    return identifier_to_signature


def split_vector(signature, bands):
    """
    Splits a signature vector into equal parts (bands).

    Args:
        signature (list): MinHash signature of a product.
        bands (int): Number of bands to split the signature into.

    Returns:
        tuple: Number of rows per band and a list of band vectors.
    """
    r = math.ceil(len(signature) / bands)  # Rows per band
    array_signature = np.array(signature)  # Convert signature to numpy array
    split_signatures = np.array_split(array_signature, bands)  # Split into bands
    return r, [list(band) for band in split_signatures]  # Return rows and bands as lists


def lsh_duplicate_detection(cleaned_data, bands):
    """
    Detects duplicate products using Locality-Sensitive Hashing (LSH).

    Args:
        cleaned_data (dict): Cleaned product data.
        bands (int): Number of bands for splitting the signature vector.

    Returns:
        set: A set of candidate pairs (product ID tuples) that are potentially duplicates.
    """
    # Step 1: Create a mapping of identifiers to signatures
    signatures = create_signatures(cleaned_data, bands)  # Generate MinHash signatures
    identifier_to_signature = id_create_signature(signatures)

    candidate_pairs = set()  # Initialize set to store candidate pairs
    buckets = [{} for _ in range(bands)]  # Initialize buckets for each band

    # Iterate over all products and process their signatures
    for identifier, signature in identifier_to_signature.items():
        # Split the signature into bands
        r, split_signatures = split_vector(signature, bands)

        # Add each band to its respective bucket
        for band_index, band_signature in enumerate(split_signatures):
            bucket_key = tuple(band_signature)  # Use band signature as the bucket key
            if bucket_key not in buckets[band_index]:
                buckets[band_index][bucket_key] = []  # Initialize a new bucket if necessary
            buckets[band_index][bucket_key].append(identifier)  # Append identifier to the bucket

    # Generate candidate pairs from the buckets
    for band_buckets in buckets:
        for bucket_key, identifiers in band_buckets.items():
            if len(identifiers) > 1:
                # Add all unique pairs of identifiers from the bucket
                for i in range(len(identifiers)):
                    for j in range(i + 1, len(identifiers)):
                        candidate_pairs.add((identifiers[i], identifiers[j]))

    return candidate_pairs

"""# MSM"""

import numpy as np
from difflib import SequenceMatcher
import re

def identifier_to_information(clean_data):
    """
    Maps product identifiers to detailed product information for easy lookup.

    Args:
        clean_data (dict): Cleaned product data.

    Returns:
        tuple:
            - A dictionary mapping product IDs to their information.
            - A dictionary mapping indices to product IDs.
    """
    identifier_to_information = {}
    index_to_identifier = {}
    identity = 0
    for key, entry in clean_data.items():
        identifier_to_information[key] = {
            "index": identity,
            "shop": entry["shop"],
            "title": entry["title"],
            "key_value_pairs": entry["featuresMap"]
        }
        index_to_identifier[identity] = key
        identity += 1

    return identifier_to_information, index_to_identifier

def exMW(nmk_i):
    """
    Extracts model words from product key-value pairs.

    Args:
        nmk_i (dict): A dictionary of product attributes.

    Returns:
        set: A set of extracted model words.
    """
    value_pattern = re.compile(r'(^\d+(\.\d+)?[a-zA-Z]+$|^\d+(\.\d+)?$)')
    model_words = set()
    for key1, value1 in nmk_i.items():
        words = value1.split()
        for word in words:
            if value_pattern.match(word):  # Identify model-like words
                model_words.add(word)
    return model_words

def extract_brand_from_title(title, brand_list):
    """
    Extracts a brand from a product title based on a predefined list of brands.

    Args:
        title (str): The product's title.
        brand_list (list): A list of known brands.

    Returns:
        str: The detected brand or None if no brand is found.
    """
    for brand in brand_list:
        if brand.lower() in title.lower():  # Case-insensitive match
            return brand.lower()
    return None

def set_inf(brand_list, info1, info2):
    """
    Checks if two products differ by brand or shop.

    Args:
        brand_list (list): List of known brands.
        info1 (dict): Product 1 information.
        info2 (dict): Product 2 information.

    Returns:
        bool: True if products are incompatible, False otherwise.
    """
    if info1.get("shop") == info2.get("shop"):  # Same shop
        return True

    # Extract brands and compare
    brand1 = extract_brand_from_title(info1["title"], brand_list)
    brand2 = extract_brand_from_title(info2["title"], brand_list)
    if brand1 is None or brand2 is None:
        return False
    return brand1 != brand2

def calculate_dissimilarity_matrix(identifier, potential_pairs, params):
    """
    Calculates a dissimilarity matrix for potential duplicate products.

    Args:
        identifier (dict): Mapping of product IDs to information.
        potential_pairs (set): Candidate pairs of products.
        params (dict): Threshold and weight parameters.

    Returns:
        tuple:
            - Dissimilarity matrix.
            - Count of comparisons performed.
    """
    n = len(identifier)
    dissimilarity_matrix = np.full((n, n), 1000, dtype=float)
    np.fill_diagonal(dissimilarity_matrix, 0)  # Diagonal represents same products

    # Predefined brand list
    brand_list = [
    "Acer Inc.", "Acer", "Admiral", "Aiwa", "Akai", "Alba", "Amstrad", "Andrea Electronics",
    "Apex Digital", "Apple Inc.", "Apple" "Arcam", "Arise India", "AGA AB", "Audiovox", "AWA",
    "Baird", "Bang & Olufsen", "Beko", "BenQ", "Binatone", "Blaupunkt", "BPL Group",
    "Brionvega", "Bush", "Canadian General Electric", "CGE" "Changhong", "ChiMei",
    "Compal Electronics", "Conar Instruments", "Continental Edison", "Cossor", "Craig",
    "Curtis Mathes Corporation", "Daewoo", "Dell", "Delmonico International Corporation",
    "DuMont Laboratories", "Durabrand", "Dynatron", "English Electric", "English Electric Valve Company",
    "EKCO", "Electrohome", "Element Electronics", "Emerson Radio & Phonograph", "EMI",
    "Farnsworth", "Ferguson Electronics", "Ferranti", "Finlux (Vestel)", "Fisher Electronics",
    "Fujitsu", "Funai", "Geloso", "General Electric", "General Electric Company", "GoldStar",
    "Goodmans Industries", "Google", "Gradiente", "Graetz", "Grundig", "Haier", "Hallicrafters",
    "Hannspree", "Heath Company/Heathkit", "Hinari Domestic Appliances", "HMV", "Hisense",
    "Hitachi", "Hoffman Television", "Itel", "ITT Corporation", "Jensen Loudspeakers", "JVC",
    "Kenmore", "Kent Television", "Kloss Video", "Kogan", "Kolster-Brandes", "Konka", "Lanix",
    "Le.com", "LG Electronics", "Loewe", "Luxor", "Magnavox", "Marantz", "Marconiphone",
    "Matsui", "Memorex", "Micromax", "Metz", "Mitsubishi", "Mivar", "Motorola", "Muntz",
    "Murphy Radio", "NEC", "Nokia", "Nordmende", "Onida", "Orion", "Orion",
    "Packard Bell", "Panasonic", "Pensonic", "Philco", "Philips", "Pioneer", "Planar Systems",
    "Polaroid", "ProLine", "ProScan", "Pye", "Pyle USA", "Quasar", "RadioShack", "Rauland-Borg",
    "RCA", "Realistic", "Rediffusion", "SABA", "Salora", "Salora International", "Samsung",
    "Sansui", "Sanyo", "Schneider Electric", "Seiki Digital", "Sèleco", "Setchell Carlson",
    "Sharp", "Siemens", "Skyworth", "Sony", "Soyo", "Stromberg-Carlson", "Supersonic",
    "Sylvania", "Symphonic", "Tandy", "Tatung Company", "TCL Corporation", "Technics", "TECO",
    "Teleavia", "Telefunken", "Teletronics", "Thomson SA", "Thorn Electrical Industries", "Thorn EMI",
    "Toshiba", "TPV Technology", "TP Vision", "Ultra", "United States Television Manufacturing Corp.",
    "Vestel", "Videocon", "Videoton", "Vizio", "Vu Televisions", "Walton", "Westinghouse Electric Corporation",
    "Westinghouse Electronics", "White-Westinghouse", "Xiaomi", "Zanussi", "Zenith Radio", "Zonda"
    ]

    number_excluded = 0
    for item1, item2 in potential_pairs:
        info1 = identifier[item1]
        info2 = identifier[item2]

        # Exclude incompatible products
        if set_inf(brand_list, info1, info2):
            dissimilarity_matrix[info1["index"], info2["index"]] = 1000
            number_excluded += 1
            continue

        # Calculate title similarity
        title_similarity = calculate_title_model_similarity(
            info1['title'], info2['title'], params['alpha'], params['beta']
        )
        hSim = title_similarity

        # Calculate dissimilarity
        dissimilarity = 1 - hSim
        index1, index2 = int(info1["index"]), int(info2["index"])
        dissimilarity_matrix[index1, index2] = dissimilarity
        dissimilarity_matrix[index2, index1] = dissimilarity  # Symmetric

    comparisons = len(potential_pairs) - number_excluded
    return dissimilarity_matrix, comparisons

def calculate_key_value_similarity(kvp1, kvp2, gamma):
    """
    Calculates the similarity between key-value pairs of two products.

    Args:
        kvp1 (dict): Key-value pairs of product 1.
        kvp2 (dict): Key-value pairs of product 2.
        gamma (float): Threshold for key similarity.

    Returns:
        tuple: Average similarity, match count, and unmatched key-value pairs.
    """
    sim = 0
    avgSim = 0
    m = 0
    w = 0
    nmk_i, nmk_j = kvp1.copy(), kvp2.copy()

    for key1, value1 in kvp1.items():
        for key2, value2 in kvp2.items():
            key_similarity = calculate_qgram_similarity(key1, key2)
            if key_similarity > gamma:
                value_similarity = calculate_qgram_similarity(value1, value2)
                weight = key_similarity
                sim += weight * value_similarity
                m += 1
                w += weight
                nmk_i.pop(key1, None)
                nmk_j.pop(key2, None)
    if w > 0:
        avgSim = sim / w
    return avgSim, m, nmk_i, nmk_j

def calculate_model_words_similarity(mw1, mw2):
    """
    Calculates similarity between model word sets.

    Args:
        mw1 (set): Model words of product 1.
        mw2 (set): Model words of product 2.

    Returns:
        float: Similarity score based on intersection over union.
    """
    intersection = len(mw1.intersection(mw2))
    union = len(mw1.union(mw2))
    return intersection / union if union > 0 else 0

def calculate_qgram_similarity(string1, string2, q=3):
    """
    Calculates q-gram similarity between two strings.

    Args:
        string1 (str): First string.
        string2 (str): Second string.
        q (int): Length of q-grams.

    Returns:
        float: Q-gram similarity score.
    """
    def qgrams(string, q):
        string = f"^{string}$"
        return [string[i:i + q] for i in range(len(string) - q + 1)]

    qgram1, qgram2 = set(qgrams(string1, q)), set(qgrams(string2, q))
    n1, n2 = len(qgram1), len(qgram2)
    qgramdistance = len(qgram1.symmetric_difference(qgram2))
    return (n1 + n2 - qgramdistance) / (n1 + n2) if (n1 + n2) > 0 else 0

def calculate_title_model_similarity(title1, title2, alpha, beta):
    """
    Calculates title similarity using Jaccard and Levenshtein metrics.

    Args:
        title1 (str): First title.
        title2 (str): Second title.
        alpha (float): Weight for Jaccard similarity.
        beta (float): Weight for Levenshtein similarity.

    Returns:
        float: Combined similarity score.
    """
    alpha, beta = alpha / (alpha + beta), beta / (alpha + beta)

    words1, words2 = set(title1.split()), set(title2.split())
    jaccard_similarity = len(words1.intersection(words2)) / len(words1.union(words2)) if len(words1.union(words2)) > 0 else 0

    levenshtein_similarity = SequenceMatcher(None, title1, title2).ratio()
    return alpha * jaccard_similarity + beta * levenshtein_similarity

"""# Clustering"""

from pickle import FALSE
import numpy as np
from sklearn.cluster import AgglomerativeClustering

def all_linkage_clustering(dissimilarity_matrix, threshold, index_to_identifier):
    """
    Performs hierarchical clustering using single, average, and complete linkage methods.

    Args:
        dissimilarity_matrix (numpy.ndarray): Precomputed dissimilarity matrix for clustering.
        threshold (float): Maximum allowed dissimilarity for products to be in the same cluster.
        index_to_identifier (list): Mapping from indices in the matrix to product identifiers.

    Returns:
        tuple: Lists of clusters (with product identifiers) for single, average, and complete linkage methods.
    """
    # Single linkage clustering
    linkage_single = AgglomerativeClustering(n_clusters=None, metric="precomputed",
                                             linkage="single", distance_threshold=threshold)
    clusters_single = linkage_single.fit_predict(dissimilarity_matrix)

    # Map cluster IDs to product identifiers for single linkage
    clusters_single_with_keys = {}
    for idx, cluster_id in enumerate(clusters_single):
        if cluster_id not in clusters_single_with_keys:
            clusters_single_with_keys[cluster_id] = []
        clusters_single_with_keys[cluster_id].append(index_to_identifier[idx])

    # Average linkage clustering
    linkage_average = AgglomerativeClustering(n_clusters=None, metric="precomputed",
                                              linkage="average", distance_threshold=threshold)
    clusters_average = linkage_average.fit_predict(dissimilarity_matrix)

    # Map cluster IDs to product identifiers for average linkage
    clusters_average_with_keys = {}
    for idx, cluster_id in enumerate(clusters_average):
        if cluster_id not in clusters_average_with_keys:
            clusters_average_with_keys[cluster_id] = []
        clusters_average_with_keys[cluster_id].append(index_to_identifier[idx])

    # Complete linkage clustering
    linkage_complete = AgglomerativeClustering(n_clusters=None, metric="precomputed",
                                               linkage="complete", distance_threshold=threshold)
    clusters_complete = linkage_complete.fit_predict(dissimilarity_matrix)

    # Map cluster IDs to product identifiers for complete linkage
    clusters_complete_with_keys = {}
    for idx, cluster_id in enumerate(clusters_complete):
        if cluster_id not in clusters_complete_with_keys:
            clusters_complete_with_keys[cluster_id] = []
        clusters_complete_with_keys[cluster_id].append(index_to_identifier[idx])

    # Return clusters for all three linkage methods
    return (list(clusters_single_with_keys.values()),
            list(clusters_average_with_keys.values()),
            list(clusters_complete_with_keys.values()))

def linkage_clustering(dissimilarity_matrix, threshold, index_to_identifier, linkage):
    """
    Performs hierarchical clustering using a specified linkage method.

    Args:
        dissimilarity_matrix (numpy.ndarray): Precomputed dissimilarity matrix for clustering.
        threshold (float): Maximum allowed dissimilarity for products to be in the same cluster.
        index_to_identifier (list): Mapping from indices in the matrix to product identifiers.
        linkage (str): Linkage method to use ('single', 'average', or 'complete').

    Returns:
        list of list: Clusters with product identifiers for the specified linkage method.
    """
    # Perform clustering using the specified linkage method
    linkage_model = AgglomerativeClustering(n_clusters=None, metric="precomputed",
                                            linkage=linkage, distance_threshold=threshold)
    clusters = linkage_model.fit_predict(dissimilarity_matrix)

    # Map cluster IDs to product identifiers
    clusters_with_keys = {}
    for idx, cluster_id in enumerate(clusters):
        if cluster_id not in clusters_with_keys:
            clusters_with_keys[cluster_id] = []
        clusters_with_keys[cluster_id].append(index_to_identifier[idx])

    return list(clusters_with_keys.values())

"""# Evaluation & bootstrapping"""

from collections import defaultdict
from itertools import combinations
import json
import numpy as np

def upload_data(bestandspad):
    """
    Loads and restructures product data from a JSON file.

    Args:
        bestandspad (str): Path to the JSON file.

    Returns:
        tuple:
            - Restructured data dictionary where keys are a combination of product number and shop.
            - List of keys representing unique product entries.
    """
    with open(bestandspad, 'r', encoding='utf-8') as file:
        data = json.load(file)
    restructured_data = {}

    for product, details in data.items():
        for shop_entry in details:
            shop = shop_entry.get("shop")
            unique_key = f"{product}|{shop}"  # Combine product number and shop
            restructured_data[unique_key] = shop_entry  # Add to restructured dictionary

    keys = list(restructured_data.keys())  # Extract all keys
    return restructured_data, keys

def calculateRecall(tp, total_combinations):
    """
    Calculates recall given true positives and total combinations.

    Args:
        tp (int): Number of true positives.
        total_combinations (int): Total possible combinations of duplicates.

    Returns:
        float: Recall score.
    """
    return tp / total_combinations if total_combinations > 0 else 0

def totalcombinations(keys):
    """
    Calculates the total number of valid combinations (TP + FN) for duplicate detection.

    Args:
        keys (list): List of unique keys combining product number and shop.

    Returns:
        int: Total number of valid combinations.
    """
    product_groups = defaultdict(list)
    for unique_key in keys:
        product_number = unique_key.split('|')[0]  # Extract product number
        product_groups[product_number].append(unique_key)

    total_combinations = 0
    for product_number, keys in product_groups.items():
        set_size = len(keys)
        if set_size > 1:
            combinations_count = (set_size * (set_size - 1)) // 2  # Calculate combinations
            total_combinations += combinations_count
    return total_combinations

def f1_score(Recall, Precission):
    """
    Computes the F1 score given recall and precision.

    Args:
        Recall (float): Recall value.
        Precission (float): Precision value.

    Returns:
        float: F1 score.
    """
    return (2 * Recall * Precission) / (Recall + Precission) if (Recall + Precission) > 0 else 0

def calculatePrecission(tp, fp):
    """
    Calculates precision given true positives and false positives.

    Args:
        tp (int): Number of true positives.
        fp (int): Number of false positives.

    Returns:
        float: Precision score.
    """
    return tp / (tp + fp) if (tp + fp) > 0 else 0

def calculate_TP_FP(clusters):
    """
    Calculates true positives (TP) and false positives (FP) from clusters.

    Args:
        clusters (list of list): List of clusters containing product identifiers.

    Returns:
        tuple:
            - TP (int): Number of true positive matches.
            - FP (int): Number of false positive matches.
    """
    TP, FP = 0, 0
    for cluster in clusters:
        if len(cluster) > 1:
            for product1, product2 in combinations(cluster, 2):
                product_number1 = product1.split('|')[0]
                product_number2 = product2.split('|')[0]

                if product_number1 == product_number2:
                    TP += 1  # True Positive
                else:
                    FP += 1  # False Positive
    return TP, FP

def bootstrapping(data, data_keys, bands):
    """
    Performs bootstrapping to evaluate metrics on multiple samples.

    Args:
        data (dict): Product data dictionary.
        data_keys (list): List of unique product keys.
        bands (int): Number of bands for LSH.

    Returns:
        dict: Averaged metrics across all bootstrap samples.
    """
    n_bootstraps = 5
    comp = []
    f1_single_scores, pair_quality_single, pair_completeness_single, f1star_single_scores = [], [], [], []
    f1_average_scores, pair_quality_average, pair_completeness_average, f1star_average_scores = [], [], [], []
    f1_complete_scores, pair_quality_complete, pair_completeness_complete, f1star_complete_scores = [], [], [], []

    for _ in range(n_bootstraps):
        sample_indices = np.random.choice(len(data_keys), len(data_keys), replace=True)
        train_indices = np.unique(sample_indices)  # 63% of the original data
        test_indices = np.setdiff1d(range(len(data_keys)), train_indices)  # Remaining 37%

        train_keys = [data_keys[i] for i in train_indices]
        test_keys = [data_keys[i] for i in test_indices]
        train_data = {key: data[key] for key in train_keys}
        test_data = {key: data[key] for key in test_keys}

        best_params_single, best_f1_single, best_threshold_single, \
        best_params_average, best_f1_average, best_threshold_average, \
        best_params_complete, best_f1_complete, best_threshold_complete = hyperparameter_tuning(train_keys, train_data, bands)

        # Generate candidate pairs and identifier mappings
        candidate_pairs = lsh_duplicate_detection(test_data, bands)
        identifier_to_info, index_to_identifier = identifier_to_information(test_data)

        # Single linkage metrics
        dissim_matrix_single, numberOfComparisonsMade = calculate_dissimilarity_matrix(identifier_to_info, candidate_pairs, best_params_single)
        clusters_single = linkage_clustering(dissim_matrix_single, best_threshold_single, index_to_identifier, "single")

        # Average linkage metrics
        dissim_matrix_average, _ = calculate_dissimilarity_matrix(identifier_to_info, candidate_pairs, best_params_average)
        clusters_average = linkage_clustering(dissim_matrix_average, best_threshold_average, index_to_identifier, "average")

        # Complete linkage metrics
        dissim_matrix_complete, _ = calculate_dissimilarity_matrix(identifier_to_info, candidate_pairs, best_params_complete)
        clusters_complete = linkage_clustering(dissim_matrix_complete, best_threshold_complete, index_to_identifier, "complete")

        # Metrics for all methods
        total_combinations = totalcombinations(test_keys)
        totalNumberOfComparisons = (len(test_keys) * (len(test_keys) - 1)) / 2

        # Calculate and store metrics for each linkage method
        for clusters, f1_scores, pair_quality, pair_completeness, f1star_scores in [
            (clusters_single, f1_single_scores, pair_quality_single, pair_completeness_single, f1star_single_scores),
            (clusters_average, f1_average_scores, pair_quality_average, pair_completeness_average, f1star_average_scores),
            (clusters_complete, f1_complete_scores, pair_quality_complete, pair_completeness_complete, f1star_complete_scores)
        ]:
            TP, FP = calculate_TP_FP(clusters)
            recall = calculateRecall(TP, total_combinations)
            quality = TP / totalNumberOfComparisons
            f1 = f1_score(recall, calculatePrecission(TP, FP))

            f1_scores.append(f1)
            pair_quality.append(quality)
            pair_completeness.append(recall)
            f1star_scores.append((2 * recall * quality) / (recall + quality) if (recall + quality) > 0 else 0)

        comp.append(numberOfComparisonsMade / totalNumberOfComparisons)

    # Calculate averages over bootstrap iterations
    return {
        "average_f1_single": np.mean(f1_single_scores),
        "average_quality_single": np.mean(pair_quality_single),
        "average_completeness_single": np.mean(pair_completeness_single),
        "average_f1star_single": np.mean(f1star_single_scores),
        "average_f1_average": np.mean(f1_average_scores),
        "average_quality_average": np.mean(pair_quality_average),
        "average_completeness_average": np.mean(pair_completeness_average),
        "average_f1star_average": np.mean(f1star_average_scores),
        "average_f1_complete": np.mean(f1_complete_scores),
        "average_quality_complete": np.mean(pair_quality_complete),
        "average_completeness_complete": np.mean(pair_completeness_complete),
        "average_f1star_complete": np.mean(f1star_complete_scores),
        "average_comp": np.mean(comp)
    }



def hyperparameter_tuning(train_keys, train_data, bands):
    """
    Finds the optimal hyperparameters for the training data by maximizing F1-scores.

    Args:
        train_keys (list): Keys of the training data (unique identifiers for products).
        train_data (dict): Training dataset containing product information.
        bands (int): Number of bands for Locality Sensitive Hashing (LSH).

    Returns:
        tuple: Optimal parameters and F1-scores for single, average, and complete linkage clustering.
        - best_params_single (dict): Best parameters for single linkage.
        - best_f1_single (float): Best F1-score for single linkage.
        - best_threshold_single (float): Best threshold for single linkage.
        - best_params_average (dict): Best parameters for average linkage.
        - best_f1_average (float): Best F1-score for average linkage.
        - best_threshold_average (float): Best threshold for average linkage.
        - best_params_complete (dict): Best parameters for complete linkage.
        - best_f1_complete (float): Best F1-score for complete linkage.
        - best_threshold_complete (float): Best threshold for complete linkage.
    """
    # Define the hyperparameter grid for tuning
    param_grid = {
        'alpha': [0.25, 0.5, 0.75, 1],  # Weight for title similarity
        'epsilon': [0.3, 0.4, 0.5, 0.6, 0.7]  # Clustering threshold
    }

    # Initialize variables to store the best parameters and scores
    best_f1_single = 0
    best_f1_average = 0
    best_f1_complete = 0

    best_params_single = None
    best_params_average = None
    best_params_complete = None

    # Generate candidate pairs using LSH
    candidate_pairs = lsh_duplicate_detection(train_data, bands)
    print(f"Number of candidate pairs: {len(candidate_pairs)}")

    # Map product identifiers to their information
    identifier_to_info, index_to_identifier = identifier_to_information(train_data)

    # Iterate over all combinations of alpha and epsilon
    for alpha in param_grid['alpha']:
        for epsilon in param_grid['epsilon']:
            # Set the current hyperparameters
            params = {
                'alpha': alpha,
                'beta': 1 - alpha,  # Ensure weights sum to 1
            }

            # Calculate the dissimilarity matrix using the current parameters
            dissim_matrix, numberOfComparisonsMade = calculate_dissimilarity_matrix(identifier_to_info, candidate_pairs, params)

            # Perform clustering with the given threshold (epsilon)
            threshold = epsilon
            clusters_single, clusters_average, clusters_complete = all_linkage_clustering(dissim_matrix, threshold, index_to_identifier)

            # Calculate the total number of true combinations for recall computation
            total_combinations = totalcombinations(train_keys)

            # Evaluate single linkage clustering
            TP_single, FP_single = calculate_TP_FP(clusters_single)
            recall_single = calculateRecall(TP_single, total_combinations)
            f1_single = f1_score(recall_single, calculatePrecission(TP_single, FP_single))

            # Evaluate average linkage clustering
            TP_average, FP_average = calculate_TP_FP(clusters_average)
            recall_average = calculateRecall(TP_average, total_combinations)
            f1_average = f1_score(recall_average, calculatePrecission(TP_average, FP_average))

            # Evaluate complete linkage clustering
            TP_complete, FP_complete = calculate_TP_FP(clusters_complete)
            recall_complete = calculateRecall(TP_complete, total_combinations)
            f1_complete = f1_score(recall_complete, calculatePrecission(TP_complete, FP_complete))

            # Update the best parameters and scores for single linkage if necessary
            if f1_single >= best_f1_single:
                best_f1_single = f1_single
                best_params_single = params
                best_threshold_single = threshold

            # Update the best parameters and scores for average linkage if necessary
            if f1_average >= best_f1_average:
                best_f1_average = f1_average
                best_params_average = params
                best_threshold_average = threshold

            # Update the best parameters and scores for complete linkage if necessary
            if f1_complete >= best_f1_complete:
                best_f1_complete = f1_complete
                best_params_complete = params
                best_threshold_complete = threshold

    # Return the best parameters, F1-scores, and thresholds for all linkage methods
    return best_params_single, best_f1_single, best_threshold_single, \
           best_params_average, best_f1_average, best_threshold_average, \
           best_params_complete, best_f1_complete, best_threshold_complete

"""# Tabel"""

import numpy as np
import pandas as pd


def main():
    # Define grid of bands to test
    bands_grid = [100, 200, 300, 400, 500, 600]  # Adjust this grid as needed

    # Path to dataset
    bestandspad = "/content/TVs-all-merged.json"

    # Load and clean data
    print("Uploading and cleaning data...")
    data, keys = upload_data(bestandspad)
    data = clean_data(data)

    # Prepare to collect results
    data_output = []

    # Process each band value
    for band in bands_grid:
        print(f"Running bootstrapping for band: {band}")
        results = bootstrapping(data, keys, band)
        data_output.append({
            "Band": band,
            "Comparisons Made / Total Comparisons": results["average_comp"],
            "F1 Score Single": results["average_f1_single"],
            "F1* Single": results["average_f1star_single"],
            "Pair Quality Single": results["average_quality_single"],
            "Pair Completeness Single": results["average_completeness_single"],
            "F1 Score Average": results["average_f1_average"],
            "F1* Average": results["average_f1star_average"],
            "Pair Quality Average": results["average_quality_average"],
            "Pair Completeness Average": results["average_completeness_average"],
            "F1 Score Complete": results["average_f1_complete"],
            "F1* Complete": results["average_f1star_complete"],
            "Pair Quality Complete": results["average_quality_complete"],
            "Pair Completeness Complete": results["average_completeness_complete"]
        })

    # Create DataFrame for results
    df = pd.DataFrame(data_output)

    # Display the results
    print("Results:")
    print(df)


if __name__ == "__main__":
    main()